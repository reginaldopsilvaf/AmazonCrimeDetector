{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "from osgeo import gdal\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_crop_data(image_dir, mask_dir, crop_size, augment=False, augment_factor=4):\n",
    "    images = []\n",
    "    masks = []\n",
    "\n",
    "    # Listar arquivos nas pastas de imagens e máscaras\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    mask_files = sorted(os.listdir(mask_dir))\n",
    "\n",
    "    # Garantir que a correspondência entre imagens e máscaras seja correta\n",
    "    assert len(image_files) == len(mask_files), \"Número de imagens e máscaras deve ser o mesmo.\"\n",
    "    \n",
    "    for img_name, mask_name in zip(image_files, mask_files):\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        mask_path = os.path.join(mask_dir, mask_name)\n",
    "\n",
    "        # Abrir a imagem e a máscara\n",
    "        img_ds = gdal.Open(img_path)\n",
    "        mask_ds = gdal.Open(mask_path)\n",
    "\n",
    "        if img_ds is None or mask_ds is None:\n",
    "            raise FileNotFoundError(f\"Erro ao abrir imagem ou máscara: {img_path}, {mask_path}\")\n",
    "\n",
    "        # Verificar se as dimensões da imagem e máscara coincidem\n",
    "        assert img_ds.RasterXSize == mask_ds.RasterXSize and img_ds.RasterYSize == mask_ds.RasterYSize, \\\n",
    "            f\"Dimensões diferentes para imagem ({img_name}) e máscara ({mask_name}).\"\n",
    "\n",
    "        img_width, img_height = img_ds.RasterXSize, img_ds.RasterYSize\n",
    "        img_bands = img_ds.RasterCount\n",
    "\n",
    "        # Iterar pelos blocos na imagem e máscara\n",
    "        for y in range(0, img_height - crop_size + 1, crop_size):\n",
    "            for x in range(0, img_width - crop_size + 1, crop_size):\n",
    "\n",
    "                # Ler bloco da imagem\n",
    "                img_block = img_ds.ReadAsArray(x, y, crop_size, crop_size) # (4, 128, 128)\n",
    "\n",
    "                # --- CORREÇÃO AQUI ---\n",
    "                # Garanta que img_block esteja sempre no formato (H, W, C)\n",
    "                if img_block.ndim == 3: # Se for multibanda (C, H, W)\n",
    "                    # Transpor para (H, W, C)\n",
    "                    img_block = np.transpose(img_block, (1, 2, 0))\n",
    "\n",
    "                elif img_block.ndim == 2: # Se for escala de cinza (H, W)\n",
    "                    # Adicionar dimensão de canal para consistência\n",
    "                    img_block = np.expand_dims(img_block, axis=-1)\n",
    "                # --- FIM DA CORREÇÃO ---\n",
    "\n",
    "                # --- ADICIONE ESTA LINHA ---\n",
    "                # Se o bloco tiver 4 canais, mantenha apenas os 3 primeiros (RGB)\n",
    "                if img_block.shape[2] == 4:\n",
    "                    img_block = img_block[:, :, :3]  # Fatiando para pegar os canais 0, 1 e 2\n",
    "                # --- FIM DA LINHA ADICIONADA ---\n",
    "\n",
    "                # Ler bloco da máscara\n",
    "                mask_block = mask_ds.ReadAsArray(x, y, crop_size, crop_size)\n",
    "                \n",
    "                # --- CORREÇÃO APLICADA AQUI ---\n",
    "                # Padroniza a forma da máscara para (H, W) antes de adicionar o canal\n",
    "                if mask_block.ndim == 3:\n",
    "                    # Se a forma for (1, H, W), remove a primeira dimensão para ficar (H, W)\n",
    "                    mask_block = np.squeeze(mask_block, axis=0)\n",
    "\n",
    "                # Garante que a máscara tenha um canal no final, resultando em (H, W, 1)\n",
    "                if mask_block.ndim == 2:\n",
    "                    mask_block = np.expand_dims(mask_block, axis=-1)\n",
    "                # --- FIM DA CORREÇÃO ---\n",
    "\n",
    "                # Normalizar os blocos\n",
    "                img_block = (img_block / 255.0).astype(np.float32)\n",
    "                mask_block = mask_block.astype(np.float32)\n",
    "                mask_block[mask_block > 0] = 1\n",
    "                # Adicionar canal extra\n",
    "                #mask_block = np.expand_dims(mask_block, axis=-1)\n",
    "\n",
    "                # Adicionar os blocos às listas\n",
    "                images.append(img_block)\n",
    "                masks.append(mask_block)\n",
    "\n",
    "                # Aplicar aumentação se solicitado\n",
    "                if augment:\n",
    "                    # A função agora retorna uma lista com 4 novas versões\n",
    "                    list_of_aug_images, list_of_aug_masks = augment_data(img_block, mask_block)\n",
    "\n",
    "                    # Usa .extend() para adicionar todas as novas versões de uma vez\n",
    "                    images.extend(list_of_aug_images)\n",
    "                    masks.extend(list_of_aug_masks)\n",
    "\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "# Função para realizar data augmentation\n",
    "def augment_data(image, mask):\n",
    "    augmented_images = []\n",
    "    augmented_masks = []\n",
    "\n",
    "    # 1. Flip Horizontal (espelhamento)\n",
    "    augmented_images.append(np.fliplr(image))\n",
    "    augmented_masks.append(np.fliplr(mask))\n",
    "\n",
    "    # 2. Flip Vertical (de cabeça para baixo)\n",
    "    augmented_images.append(np.flipud(image))\n",
    "    augmented_masks.append(np.flipud(mask))\n",
    "\n",
    "    # 3. Rotação de 90 graus (sentido anti-horário)\n",
    "    augmented_images.append(np.rot90(image, k=1))\n",
    "    augmented_masks.append(np.rot90(mask, k=1))\n",
    "    \n",
    "    # 4. Rotação de 270 graus (ou -90 graus)\n",
    "    augmented_images.append(np.rot90(image, k=3))\n",
    "    augmented_masks.append(np.rot90(mask, k=3))\n",
    "    \n",
    "    return augmented_images, augmented_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net simplificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net simplificada\n",
    "def build_unet(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    u1 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c3)\n",
    "    u1 = layers.concatenate([u1, c2])\n",
    "    c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u1)\n",
    "    c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    u2 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c4)\n",
    "    u2 = layers.concatenate([u2, c1])\n",
    "    c5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u2)\n",
    "    c5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# --- COMO USAR ---\n",
    "\n",
    "# 1. Construa o modelo\n",
    "input_shape = (128, 128, 3)\n",
    "model = build_unet(input_shape=input_shape)\n",
    "\n",
    "# 3. Veja o resultado\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net com encoder pré-treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Input\n",
    "\n",
    "def build_unet_mobilenetv2(input_shape, num_classes=1):\n",
    "    \"\"\"\n",
    "    Constrói uma arquitetura U-Net usando um encoder VGG16 pré-treinado.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): O tamanho dos patches de entrada (altura, largura, canais).\n",
    "        num_classes (int): O número de classes de saída. Para segmentação binária, use 1.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: O modelo U-Net compilado.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. CARREGAR O ENCODER (BACKBONE) VGG16 PRÉ-TREINADO\n",
    "    # include_top=False remove as camadas de classificação no final.\n",
    "    # weights='imagenet' carrega os pesos aprendidos com o dataset ImageNet.\n",
    "    backbone = keras.applications.MobileNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    # Congelar os pesos do encoder para que eles não sejam treinados inicialmente.\n",
    "    # Vamos apenas treinar nosso novo decoder.\n",
    "    backbone.trainable = False\n",
    "\n",
    "    # 2. IDENTIFICAR AS CAMADAS DE SKIP CONNECTION DO ENCODER\n",
    "    # Precisamos das saídas das camadas de Max-Pooling do VGG16 para conectar ao decoder.\n",
    "    # Você pode ver os nomes das camadas rodando `backbone.summary()`.\n",
    "    skip_connections_names = [\n",
    "        'block_1_expand_relu',  # 64x64\n",
    "        'block_3_expand_relu',  # 32x32\n",
    "        'block_6_expand_relu',  # 16x16\n",
    "        'block_13_expand_relu',  # 8x8\n",
    "    ]\n",
    "    # Pega a saída (tensor) de cada uma dessas camadas.\n",
    "    encoder_outputs = [backbone.get_layer(name).output for name in skip_connections_names]\n",
    "    \n",
    "    # A entrada para o decoder será a saída final do encoder.\n",
    "    encoder_final_output = backbone.output # 4x4\n",
    "\n",
    "    # 3. CONSTRUIR O DECODER (CAMINHO DE EXPANSÃO)\n",
    "    # Vamos subir, aumentando a resolução e concatenando com as skip connections.\n",
    "    \n",
    "    # Bloco expansivo 1\n",
    "    # Sobe de 4x4 para 8x8\n",
    "    up_stack_1 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(encoder_final_output)\n",
    "    concat_1 = Concatenate()([up_stack_1, encoder_outputs[3]]) # Conecta com a saída do block4_pool\n",
    "    conv_stack_1 = Conv2D(128, 3, activation='relu', padding='same')(concat_1)\n",
    "    conv_stack_1 = Conv2D(128, 3, activation='relu', padding='same')(conv_stack_1)\n",
    "\n",
    "    # Bloco expansivo 2\n",
    "    # Sobe de 8x8 para 16x16\n",
    "    up_stack_2 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv_stack_1)\n",
    "    concat_2 = Concatenate()([up_stack_2, encoder_outputs[2]]) # Conecta com a saída do block3_pool\n",
    "    conv_stack_2 = Conv2D(64, 3, activation='relu', padding='same')(concat_2)\n",
    "    conv_stack_2 = Conv2D(64, 3, activation='relu', padding='same')(conv_stack_2)\n",
    "\n",
    "    # Bloco expansivo 3\n",
    "    # Sobe de 16x16 para 32x32\n",
    "    up_stack_3 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv_stack_2)\n",
    "    concat_3 = Concatenate()([up_stack_3, encoder_outputs[1]]) # Conecta com a saída do block2_pool\n",
    "    conv_stack_3 = Conv2D(32, 3, activation='relu', padding='same')(concat_3)\n",
    "    conv_stack_3 = Conv2D(32, 3, activation='relu', padding='same')(conv_stack_3)\n",
    "\n",
    "    # Bloco expansivo 4\n",
    "    # Sobe de 32x32 para 64x64\n",
    "    up_stack_4 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(conv_stack_3)\n",
    "    concat_4 = Concatenate()([up_stack_4, encoder_outputs[0]]) # Conecta com a saída do block1_pool\n",
    "    conv_stack_4 = Conv2D(16, 3, activation='relu', padding='same')(concat_4)\n",
    "    conv_stack_4 = Conv2D(16, 3, activation='relu', padding='same')(conv_stack_4)\n",
    "    \n",
    "    # Bloco expansivo 5 (final para restaurar o tamanho original)\n",
    "    # Sobe de 64x64 para 128x128\n",
    "    up_stack_5 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same')(conv_stack_4)\n",
    "    conv_stack_5 = Conv2D(8, 3, activation='relu', padding='same')(up_stack_5)\n",
    "    conv_stack_5 = Conv2D(8, 3, activation='relu', padding='same')(conv_stack_5)\n",
    "\n",
    "\n",
    "    # 4. CAMADA DE SAÍDA\n",
    "    # Usa um filtro com o número de classes e a ativação apropriada.\n",
    "    # Para segmentação binária, é 1 classe com ativação 'sigmoid'.\n",
    "    output_layer = Conv2D(num_classes, 1, activation='sigmoid')(conv_stack_5)\n",
    "\n",
    "    # 5. CRIAR E RETORNAR O MODELO FINAL\n",
    "    # A entrada do modelo é a entrada do backbone VGG16.\n",
    "    model = keras.Model(inputs=backbone.input, outputs=output_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- COMO USAR ---\n",
    "\n",
    "# 1. Construa o modelo\n",
    "input_shape = (128, 128, 3)\n",
    "model = build_unet_mobilenetv2(input_shape=input_shape)\n",
    "\n",
    "# 3. Veja o resultado\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred):\n",
    "    # Garantir que ambos os tensores sejam do tipo float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)  # Binarizar y_pred com threshold de 0.5\n",
    "    \n",
    "    # Calcular interseção e união\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    \n",
    "    # Prevenir divisão por zero\n",
    "    return tf.math.divide_no_nan(intersection, union)\n",
    "\n",
    "def dice_coef(y_true, y_pred, threshold=0.5, epsilon=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > threshold, tf.float32)\n",
    "    y_true_flat = keras.layers.Flatten()(y_true)\n",
    "    y_pred_flat = keras.layers.Flatten()(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true_flat * y_pred_flat)\n",
    "    return (2. * intersection + epsilon) / (tf.reduce_sum(y_true_flat) + tf.reduce_sum(y_pred_flat) + epsilon)\n",
    "\n",
    "def specificity(y_true, y_pred, threshold=0.5, epsilon=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > threshold, tf.float32)\n",
    "    tn = tf.reduce_sum((1 - y_true) * (1 - y_pred))\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    return tn / (tn + fp + epsilon)\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    dice_coefficient = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - dice_coefficient # A perda é 1 - o coeficiente\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred, pos_weight=100.):\n",
    "    y_true = K.cast(y_true, tf.float32)\n",
    "    \n",
    "    # Calcula a cross-entropy\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Aplica os pesos\n",
    "    weight_vector = y_true * pos_weight + (1. - y_true)\n",
    "    weighted_bce = weight_vector * bce\n",
    "    \n",
    "    return K.mean(weighted_bce)\n",
    "\n",
    "def combined_loss(y_true, y_pred, alpha=0.3):\n",
    "    return alpha * weighted_binary_crossentropy(y_true, y_pred) + (1 - alpha) * dice_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "def train_model(train_image_dir, train_mask_dir, epochs, crop_size, batch_size, lr, gamma, experiment_name, run_name, model_name, saving_dir, augment):\n",
    "    # Carregar, recortar e aplicar data augmentation nos dados de treinamento\n",
    "    print(\"Carregando, recortando e aplicando data augmentation nos dados de treinamento...\")\n",
    "    start_time = time.time()\n",
    "    train_images, train_masks = load_and_crop_data(train_image_dir, train_mask_dir, crop_size=crop_size, augment=augment)\n",
    "    print(f\"Dados de treinamento carregados e aumentados em {time.time() - start_time:.2f} segundos\")\n",
    "    print(f\"Número total de imagens: {len(train_images)}\")\n",
    "\n",
    "    # Dividir os dados em 70% treino e 30% validação\n",
    "    train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        train_images, train_masks, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Criar datasets para treinamento e validação\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_masks))\n",
    "    train_dataset = train_dataset.shuffle(len(train_images)).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_masks))\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    print(f\"Treinamento: {len(train_images)} imagens\")\n",
    "    print(f\"Validação: {len(val_images)} imagens\")\n",
    "\n",
    "    # Construir modelo\n",
    "    print(\"Construindo o modelo...\")\n",
    "    input_shape = (crop_size, crop_size, 3)  # Imagens recortadas\n",
    "    model = build_unet_mobilenetv2(input_shape)\n",
    "    print(\"Modelo construído!\")\n",
    "\n",
    "    mlflow.set_tracking_uri('http://localhost:5000')\n",
    "\n",
    "    # Defina um nome para o seu experimento no MLflow\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # >>>>> A MÁGICA ACONTECE AQUI <<<<<\n",
    "    # Ative o autologging para Keras\n",
    "    mlflow.keras.autolog()\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "        # Defina as métricas que o Keras irá calcular (e o MLflow irá capturar)\n",
    "        metrics_to_track = [\n",
    "            keras.metrics.BinaryIoU(threshold=0.5, name='iou'),\n",
    "            keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy'),\n",
    "            keras.metrics.Precision(thresholds=0.5, name='precision'),\n",
    "            keras.metrics.Recall(thresholds=0.5, name='recall'),\n",
    "            dice_coef,\n",
    "            specificity\n",
    "        ]\n",
    "\n",
    "        # Compilar modelo com Focal Loss\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=combined_loss, \n",
    "                    metrics=metrics_to_track\n",
    "                    )\n",
    "\n",
    "        # Configurar callback para ajuste do learning rate\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            monitor='val_iou',   # Monitorar o iou no conjunto de validação\n",
    "            factor=0.5,          # Fator de redução da taxa de aprendizado\n",
    "            patience=5,          # Número de épocas sem melhora antes de reduzir\n",
    "            min_lr=1e-6,         # Limite inferior para o learning rate\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Configurar callback para salvar modelo\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            saving_dir+'/'+f'{model_name}.keras',\n",
    "            monitor='val_iou',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "\n",
    "        )\n",
    "\n",
    "        # Configurar callback para early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_iou',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "\n",
    "        )\n",
    "\n",
    "        # Treinar modelo\n",
    "        print(\"Iniciando o treinamento...\")\n",
    "        print(\"Iniciando run do MLflow:\", run.info.run_id)\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.fit(\n",
    "            train_dataset, \n",
    "            validation_data=val_dataset,  # Adiciona o conjunto de validação\n",
    "            epochs=epochs, \n",
    "            verbose=1,\n",
    "            callbacks=[lr_scheduler, model_checkpoint, early_stopping]\n",
    "        )\n",
    "\n",
    "    print(f\"Treinamento concluído em {time.time() - start_time:.2f} segundos\")\n",
    "    print(\"Carregando o melhor modelo salvo do arquivo 'melhor_modelo.keras'...\")\n",
    "\n",
    "    # Não se esqueça de passar seus objetos customizados para que o Keras os reconheça!\n",
    "    custom_objects = {\n",
    "        'combined_loss': combined_loss, \n",
    "        'dice_loss': dice_loss, \n",
    "        'weighted_binary_crossentropy': weighted_binary_crossentropy,\n",
    "        'iou': iou,\n",
    "        'dice_coef': dice_coef,\n",
    "        'specificity': specificity\n",
    "    }\n",
    "\n",
    "    # Carrega o melhor modelo que foi salvo durante o treinamento\n",
    "    best_model = load_model(saving_dir+'/'+f'{model_name}.keras', custom_objects=custom_objects)\n",
    "\n",
    "    print(f\"Salvando o modelo no formato de pasta (TensorFlow SavedModel) em '{saving_dir}'...\")\n",
    "\n",
    "    # Salva o modelo novamente, mas desta vez sem extensão, para criar a pasta\n",
    "    best_model.export(saving_dir+'/'+f'{model_name}')\n",
    "\n",
    "    print(\"Processo concluído com sucesso!\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificação dos dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição completa da classe DataGenerator\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_dir, mask_dir, crop_size, batch_size, augment=False):\n",
    "        self.image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir)])\n",
    "        self.mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir)])\n",
    "        self.crop_size = crop_size\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augment\n",
    "        \n",
    "        self.all_patches = self._create_patch_list()\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def _create_patch_list(self):\n",
    "        patch_coords = []\n",
    "        for i, img_path in enumerate(self.image_files):\n",
    "            try:\n",
    "                img_ds = gdal.Open(img_path)\n",
    "                if img_ds is None:\n",
    "                    print(f\"Aviso: Não foi possível abrir a imagem {img_path}. Pulando.\")\n",
    "                    continue\n",
    "                img_width, img_height = img_ds.RasterXSize, img_ds.RasterYSize\n",
    "                \n",
    "                for y in range(0, img_height - self.crop_size + 1, self.crop_size):\n",
    "                    for x in range(0, img_width - self.crop_size + 1, self.crop_size):\n",
    "                        patch_coords.append({'file_index': i, 'x': x, 'y': y})\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar o arquivo {img_path}: {e}\")\n",
    "        return patch_coords\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.all_patches) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_patch_indices = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        patches_to_load = [self.all_patches[i] for i in batch_patch_indices]\n",
    "        X, y = self._data_generation(patches_to_load)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.all_patches))\n",
    "        np.random.shuffle(self.indexes)\n",
    "\n",
    "    def _data_generation(self, patches_to_load):\n",
    "        X = np.empty((self.batch_size, self.crop_size, self.crop_size, 3), dtype=np.float32)\n",
    "        y = np.empty((self.batch_size, self.crop_size, self.crop_size, 1), dtype=np.float32)\n",
    "\n",
    "        for i, patch_info in enumerate(patches_to_load):\n",
    "            file_idx = patch_info['file_index']\n",
    "            x, y_coord = patch_info['x'], patch_info['y']\n",
    "            \n",
    "            img_ds = gdal.Open(self.image_files[file_idx])\n",
    "            mask_ds = gdal.Open(self.mask_files[file_idx])\n",
    "\n",
    "            img_block = img_ds.ReadAsArray(x, y_coord, self.crop_size, self.crop_size)\n",
    "            mask_block = mask_ds.ReadAsArray(x, y_coord, self.crop_size, self.crop_size)\n",
    "\n",
    "            img_block = np.transpose(img_block, (1, 2, 0))\n",
    "            if img_block.shape[2] == 4:\n",
    "                img_block = img_block[:, :, :3]\n",
    "            \n",
    "            if mask_block.ndim == 2:\n",
    "                mask_block = np.expand_dims(mask_block, axis=-1)\n",
    "\n",
    "            img_block = (img_block / 255.0).astype(np.float32)\n",
    "            mask_block = (mask_block).astype(np.float32)\n",
    "            \n",
    "            if self.augment:\n",
    "                img_block, mask_block = augment_data(img_block, mask_block)\n",
    "\n",
    "            X[i,] = img_block\n",
    "            y[i,] = mask_block\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- Configure os seus parâmetros aqui ---\n",
    "# (Use os mesmos valores que você usa para o treinamento)\n",
    "crop_size = 128\n",
    "batch_size = 8 # Pode ser qualquer valor, ex: 8, para visualização\n",
    "augment = False # Mantenha como False para uma verificação limpa dos dados originais\n",
    "\n",
    "# --- Fim da Configuração ---\n",
    "\n",
    "\n",
    "# 1. Crie uma instância do seu DataGenerator\n",
    "# (Assumindo que a classe DataGenerator que definimos anteriormente está disponível)\n",
    "try:\n",
    "    train_generator = DataGenerator(\n",
    "        image_dir=train_image_dir,\n",
    "        mask_dir=train_mask_dir,\n",
    "        crop_size=crop_size,\n",
    "        batch_size=batch_size,\n",
    "        augment=augment\n",
    "    )\n",
    "except NameError:\n",
    "    print(\"ERRO: A classe 'DataGenerator' não foi encontrada. Certifique-se de que ela está definida no seu script.\")\n",
    "    # Se der este erro, pare e cole a definição da classe DataGenerator aqui.\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: Verifique se os caminhos '{train_image_dir}' e '{train_mask_dir}' estão corretos.\")\n",
    "    # Se der este erro, corrija os caminhos na seção de configuração acima.\n",
    "\n",
    "\n",
    "# 2. Pega o primeiro lote de dados gerado\n",
    "print(\"Gerando o primeiro lote de dados para visualização...\")\n",
    "images, masks = train_generator[0] \n",
    "\n",
    "# 3. Imprime informações de diagnóstico (Sanity Check)\n",
    "print(f\"Forma (shape) do lote de imagens: {images.shape}\") # Deve ser (batch_size, 128, 128, 3)\n",
    "print(f\"Forma (shape) do lote de máscaras: {masks.shape}\") # Deve ser (batch_size, 128, 128, 1)\n",
    "print(f\"Tipo de dados das imagens: {images.dtype}\") # Deve ser float32\n",
    "print(f\"Tipo de dados das máscaras: {masks.dtype}\") # Deve ser float32\n",
    "print(f\"Valores únicos na primeira máscara (deveria ser 0 e 1): {np.unique(masks[0])}\")\n",
    "\n",
    "\n",
    "# 4. Plota as imagens e máscaras\n",
    "print(\"\\nPlotando as imagens e máscaras...\")\n",
    "n_samples = min(5, batch_size)  # Mostra até 5 amostras do lote\n",
    "\n",
    "plt.figure(figsize=(10, n_samples * 3))\n",
    "for i in range(n_samples):\n",
    "    # Plota a imagem original\n",
    "    plt.subplot(n_samples, 2, 2 * i + 1)\n",
    "    # A imagem foi normalizada (dividida por 255), então ela pode parecer escura ou estranha, isso é normal.\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(f\"Imagem de Treino {i}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plota a máscara correspondente\n",
    "    plt.subplot(n_samples, 2, 2 * i + 2)\n",
    "    # Usamos .squeeze() para remover a dimensão do canal (128, 128, 1) -> (128, 128)\n",
    "    # Usamos cmap='gray' para garantir a visualização em preto e branco\n",
    "    plt.imshow(masks[i].squeeze(), cmap='gray')\n",
    "    plt.title(f\"Máscara de Treino {i}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criar experimento no MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri='http://127.0.0.1:5000')\n",
    "\n",
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"Projeto de detecção de crimes transfronteiriços na Amazônia. \"\n",
    "    \"Este experimento contém os modelos de detecção de áreas com indícios de pistas de pouso ilegais\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"nome_projeto\":\"Identificar Crimes Transfronteiriços na Amazônia\",\n",
    "    \"tipo_crime\": \"Pistas de pouso\",\n",
    "    \"mlflow.note.content\": experiment_description\n",
    "\n",
    "}\n",
    "\n",
    "# Create the Experiment, providing a unique name\n",
    "garimpos_experiment = client.create_experiment(\n",
    "    name=\"Modelos_pistas\", tags=experiment_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 128\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "gamma = 2\n",
    "augment=True\n",
    "\n",
    "# Caminhos de dados\n",
    "train_image_dir = \"d:/SIMGEO 2025/DATASETS/GERAL/IMGS\"\n",
    "train_mask_dir = \"d:/SIMGEO 2025/DATASETS/GERAL/MASKS\"\n",
    "\n",
    "model = train_model(train_image_dir=train_image_dir, \n",
    "                    train_mask_dir=train_mask_dir, \n",
    "                    epochs=epochs, \n",
    "                    crop_size=crop_size, \n",
    "                    batch_size=batch_size, \n",
    "                    lr=lr, \n",
    "                    gamma=gamma,\n",
    "                    experiment_name='Modelos_gerais',\n",
    "                    run_name='geral_teste1', \n",
    "                    model_name='modelo_melhor_treino_gerais', \n",
    "                    saving_dir='C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet',\n",
    "                    augment=augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_georeferenced_mask(predicted_mask, reference_image_path, output_path, threshold):\n",
    "\n",
    "    # Abrir a imagem de referência para obter informações georreferenciadas\n",
    "    ref_ds = gdal.Open(reference_image_path)\n",
    "    if ref_ds is None:\n",
    "        raise FileNotFoundError(f\"Não foi possível abrir a imagem de referência: {reference_image_path}\")\n",
    "    \n",
    "    # Obter informações georreferenciadas\n",
    "    geo_transform = ref_ds.GetGeoTransform()\n",
    "    projection = ref_ds.GetProjection()\n",
    "    ref_ds = None  # Fechar o dataset da referência\n",
    "    \n",
    "    # Binarizar a máscara predita com um limite (threshold) de 0.5\n",
    "    binary_mask = (predicted_mask >= threshold).astype(np.uint8)\n",
    "\n",
    "    # Criar um arquivo TIFF georreferenciado\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    out_ds = driver.Create(\n",
    "        output_path, \n",
    "        binary_mask.shape[1], \n",
    "        binary_mask.shape[0], \n",
    "        1,  # Apenas um canal\n",
    "        gdal.GDT_Byte  # Tipo de dado Byte (0 ou 1)\n",
    "    )\n",
    "    if out_ds is None:\n",
    "        raise RuntimeError(f\"Não foi possível criar o arquivo: {output_path}\")\n",
    "    \n",
    "    # Aplicar informações georreferenciadas\n",
    "    out_ds.SetGeoTransform(geo_transform)\n",
    "    out_ds.SetProjection(projection)\n",
    "    \n",
    "    # Escrever a máscara binária no arquivo\n",
    "    out_ds.GetRasterBand(1).WriteArray(binary_mask)\n",
    "    \n",
    "    # Fechar o dataset de saída\n",
    "    out_ds.FlushCache()\n",
    "    out_ds = None\n",
    "    \n",
    "    print(f\"Máscara predita salva como TIFF georreferenciado em: {output_path}\")\n",
    "\n",
    "def crop_image(image_path, crop_size):\n",
    "    # Abrir a imagem com GDAL\n",
    "    img_ds = gdal.Open(image_path)\n",
    "    if img_ds is None:\n",
    "        raise FileNotFoundError(f\"Não foi possível abrir a imagem: {image_path}\")\n",
    "\n",
    "    # Dimensões da imagem\n",
    "    img_width = img_ds.RasterXSize\n",
    "    img_height = img_ds.RasterYSize\n",
    "    img_bands = img_ds.RasterCount\n",
    "\n",
    "    # Inicializar listas para armazenar os blocos e coordenadas\n",
    "    crops = []\n",
    "    coords = []\n",
    "\n",
    "    # Iterar sobre a imagem em blocos de crop_size x crop_size\n",
    "    for y in range(0, img_height - crop_size + 1, crop_size):\n",
    "        for x in range(0, img_width - crop_size + 1, crop_size):\n",
    "            if img_bands > 3:  # Ajustar para imagens multibanda\n",
    "                crop = np.stack(\n",
    "                    [\n",
    "                        img_ds.GetRasterBand(1).ReadAsArray(x, y, crop_size, crop_size),  # Banda 1 (R)\n",
    "                        img_ds.GetRasterBand(2).ReadAsArray(x, y, crop_size, crop_size),  # Banda 2 (G)\n",
    "                        img_ds.GetRasterBand(3).ReadAsArray(x, y, crop_size, crop_size),  # Banda 3 (B)\n",
    "                    ],\n",
    "                    axis=-1,  # Combinar no formato (H, W, C)\n",
    "                )\n",
    "\n",
    "            # Adicionar o bloco e suas coordenadas à lista\n",
    "            crops.append(crop)\n",
    "            coords.append((y, x))\n",
    "\n",
    "    # Fechar o dataset GDAL\n",
    "    img_ds = None\n",
    "\n",
    "    return np.array(crops), coords\n",
    "\n",
    "def reconstruct_image(predicted_crops, coords, original_shape, crop_size):\n",
    "    # Criar matriz para reconstrução e mapa de contagem\n",
    "    reconstructed = np.zeros(original_shape[:2], dtype=np.float32)\n",
    "    count_map = np.zeros(original_shape[:2], dtype=np.float32)\n",
    "\n",
    "    # Combinar blocos preditos na posição correta\n",
    "    for (i, j), crop in zip(coords, predicted_crops):\n",
    "        reconstructed[i:i+crop_size, j:j+crop_size] += crop.squeeze()\n",
    "        count_map[i:i+crop_size, j:j+crop_size] += 1\n",
    "\n",
    "    return reconstructed\n",
    "\n",
    "def predict_and_save(test_imgs_dir, model, model_save_path, crop_size, threshold, save_model=False):\n",
    "\n",
    "    imgs = os.listdir(test_imgs_dir)\n",
    "\n",
    "    for img in imgs:\n",
    "        if not img.endswith('.tif'):\n",
    "            print(f'O arquivo {img} não é uma imagem TIF')\n",
    "        else:\n",
    "            test_img_path = os.path.join(test_imgs_dir, img)\n",
    "\n",
    "            # Carregar e recortar a imagem de teste\n",
    "            test_image_crops, test_coords = crop_image(test_img_path, crop_size=crop_size)\n",
    "            test_image = cv2.imread(test_img_path) / 255.0\n",
    "            \n",
    "            # Prever cada bloco\n",
    "            predicted_crops = model.predict(test_image_crops)\n",
    "\n",
    "            # Reconstruir a máscara predita no tamanho original\n",
    "            predicted_mask_full = reconstruct_image(predicted_crops, test_coords, test_image.shape, crop_size=crop_size)\n",
    "            print(max(np.unique(predicted_mask_full)), min(np.unique(predicted_mask_full)))\n",
    "            # Salvar a máscara predita\n",
    "            predicted_mask_path = f\"D:/ORF/dados-rf-cma/pista/resultados-testes/predicted_mask_{img}\"\n",
    "            save_georeferenced_mask(predicted_mask_full, test_img_path, predicted_mask_path, threshold=threshold)\n",
    "    # if save_model == True:\n",
    "    #     # Salvar o modelo treinado\n",
    "    #     model.save(model_save_path+'.')\n",
    "    #     print(f\"Modelo treinado salvo em: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_save(test_imgs_dir=test_image_path, model=model, model_save_path=model_save_path, crop_size=crop_size, threshold=0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simgeo_projeto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "from osgeo import gdal\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_crop_data(image_dir, mask_dir, crop_size, augment=False, augment_factor=4):\n",
    "    images = []\n",
    "    masks = []\n",
    "\n",
    "    # Listar arquivos nas pastas de imagens e máscaras\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    mask_files = sorted(os.listdir(mask_dir))\n",
    "\n",
    "    # Garantir que a correspondência entre imagens e máscaras seja correta\n",
    "    assert len(image_files) == len(mask_files), \"Número de imagens e máscaras deve ser o mesmo.\"\n",
    "    \n",
    "    for img_name, mask_name in zip(image_files, mask_files):\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        mask_path = os.path.join(mask_dir, mask_name)\n",
    "\n",
    "        # Abrir a imagem e a máscara\n",
    "        img_ds = gdal.Open(img_path)\n",
    "        mask_ds = gdal.Open(mask_path)\n",
    "\n",
    "        if img_ds is None or mask_ds is None:\n",
    "            raise FileNotFoundError(f\"Erro ao abrir imagem ou máscara: {img_path}, {mask_path}\")\n",
    "\n",
    "        # Verificar se as dimensões da imagem e máscara coincidem\n",
    "        assert img_ds.RasterXSize == mask_ds.RasterXSize and img_ds.RasterYSize == mask_ds.RasterYSize, \\\n",
    "            f\"Dimensões diferentes para imagem ({img_name}) e máscara ({mask_name}).\"\n",
    "\n",
    "        img_width, img_height = img_ds.RasterXSize, img_ds.RasterYSize\n",
    "        img_bands = img_ds.RasterCount\n",
    "\n",
    "        # Iterar pelos blocos na imagem e máscara\n",
    "        for y in range(0, img_height - crop_size + 1, crop_size):\n",
    "            for x in range(0, img_width - crop_size + 1, crop_size):\n",
    "\n",
    "                # Ler bloco da imagem\n",
    "                img_block = img_ds.ReadAsArray(x, y, crop_size, crop_size) # (4, 128, 128)\n",
    "\n",
    "                # --- CORREÇÃO AQUI ---\n",
    "                # Garanta que img_block esteja sempre no formato (H, W, C)\n",
    "                if img_block.ndim == 3: # Se for multibanda (C, H, W)\n",
    "                    # Transpor para (H, W, C)\n",
    "                    img_block = np.transpose(img_block, (1, 2, 0))\n",
    "\n",
    "                elif img_block.ndim == 2: # Se for escala de cinza (H, W)\n",
    "                    # Adicionar dimensão de canal para consistência\n",
    "                    img_block = np.expand_dims(img_block, axis=-1)\n",
    "                # --- FIM DA CORREÇÃO ---\n",
    "\n",
    "                # --- ADICIONE ESTA LINHA ---\n",
    "                # Se o bloco tiver 4 canais, mantenha apenas os 3 primeiros (RGB)\n",
    "                if img_block.shape[2] == 4:\n",
    "                    img_block = img_block[:, :, :3]  # Fatiando para pegar os canais 0, 1 e 2\n",
    "                # --- FIM DA LINHA ADICIONADA ---\n",
    "\n",
    "                # Ler bloco da máscara\n",
    "                mask_block = mask_ds.ReadAsArray(x, y, crop_size, crop_size)\n",
    "                \n",
    "                # --- CORREÇÃO APLICADA AQUI ---\n",
    "                # Padroniza a forma da máscara para (H, W) antes de adicionar o canal\n",
    "                if mask_block.ndim == 3:\n",
    "                    # Se a forma for (1, H, W), remove a primeira dimensão para ficar (H, W)\n",
    "                    mask_block = np.squeeze(mask_block, axis=0)\n",
    "\n",
    "                # Garante que a máscara tenha um canal no final, resultando em (H, W, 1)\n",
    "                if mask_block.ndim == 2:\n",
    "                    mask_block = np.expand_dims(mask_block, axis=-1)\n",
    "                # --- FIM DA CORREÇÃO ---\n",
    "\n",
    "                # Normalizar os blocos\n",
    "                img_block = (img_block / 255.0).astype(np.float32)\n",
    "                mask_block = mask_block.astype(np.float32)\n",
    "                mask_block[mask_block > 0] = 1\n",
    "                # Adicionar canal extra\n",
    "                #mask_block = np.expand_dims(mask_block, axis=-1)\n",
    "\n",
    "                # Adicionar os blocos às listas\n",
    "                images.append(img_block)\n",
    "                masks.append(mask_block)\n",
    "\n",
    "                # Aplicar aumentação se solicitado\n",
    "                if augment:\n",
    "                    # A função agora retorna uma lista com 4 novas versões\n",
    "                    list_of_aug_images, list_of_aug_masks = augment_data(img_block, mask_block)\n",
    "\n",
    "                    # Usa .extend() para adicionar todas as novas versões de uma vez\n",
    "                    images.extend(list_of_aug_images)\n",
    "                    masks.extend(list_of_aug_masks)\n",
    "\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "# Função para realizar data augmentation\n",
    "def augment_data(image, mask):\n",
    "    augmented_images = []\n",
    "    augmented_masks = []\n",
    "\n",
    "    # 1. Flip Horizontal (espelhamento)\n",
    "    augmented_images.append(np.fliplr(image))\n",
    "    augmented_masks.append(np.fliplr(mask))\n",
    "\n",
    "    # 2. Flip Vertical (de cabeça para baixo)\n",
    "    augmented_images.append(np.flipud(image))\n",
    "    augmented_masks.append(np.flipud(mask))\n",
    "\n",
    "    # 3. Rotação de 90 graus (sentido anti-horário)\n",
    "    augmented_images.append(np.rot90(image, k=1))\n",
    "    augmented_masks.append(np.rot90(mask, k=1))\n",
    "    \n",
    "    # 4. Rotação de 270 graus (ou -90 graus)\n",
    "    augmented_images.append(np.rot90(image, k=3))\n",
    "    augmented_masks.append(np.rot90(mask, k=3))\n",
    "    \n",
    "    return augmented_images, augmented_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net simplificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net simplificada\n",
    "def build_unet(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    u1 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c3)\n",
    "    u1 = layers.concatenate([u1, c2])\n",
    "    c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u1)\n",
    "    c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    u2 = layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c4)\n",
    "    u2 = layers.concatenate([u2, c1])\n",
    "    c5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u2)\n",
    "    c5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# --- COMO USAR ---\n",
    "\n",
    "# 1. Construa o modelo\n",
    "input_shape = (128, 128, 3)\n",
    "model = build_unet(input_shape=input_shape)\n",
    "\n",
    "# 3. Veja o resultado\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net com encoder pré-treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, Input\n",
    "\n",
    "def build_unet_mobilenetv2(input_shape, num_classes=1):\n",
    "    \"\"\"\n",
    "    Constrói uma arquitetura U-Net usando um encoder VGG16 pré-treinado.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): O tamanho dos patches de entrada (altura, largura, canais).\n",
    "        num_classes (int): O número de classes de saída. Para segmentação binária, use 1.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: O modelo U-Net compilado.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. CARREGAR O ENCODER (BACKBONE) VGG16 PRÉ-TREINADO\n",
    "    # include_top=False remove as camadas de classificação no final.\n",
    "    # weights='imagenet' carrega os pesos aprendidos com o dataset ImageNet.\n",
    "    backbone = keras.applications.MobileNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    # Congelar os pesos do encoder para que eles não sejam treinados inicialmente.\n",
    "    # Vamos apenas treinar nosso novo decoder.\n",
    "    backbone.trainable = False\n",
    "\n",
    "    # 2. IDENTIFICAR AS CAMADAS DE SKIP CONNECTION DO ENCODER\n",
    "    # Precisamos das saídas das camadas de Max-Pooling do VGG16 para conectar ao decoder.\n",
    "    # Você pode ver os nomes das camadas rodando `backbone.summary()`.\n",
    "    skip_connections_names = [\n",
    "        'block_1_expand_relu',  # 64x64\n",
    "        'block_3_expand_relu',  # 32x32\n",
    "        'block_6_expand_relu',  # 16x16\n",
    "        'block_13_expand_relu',  # 8x8\n",
    "    ]\n",
    "    # Pega a saída (tensor) de cada uma dessas camadas.\n",
    "    encoder_outputs = [backbone.get_layer(name).output for name in skip_connections_names]\n",
    "    \n",
    "    # A entrada para o decoder será a saída final do encoder.\n",
    "    encoder_final_output = backbone.output # 4x4\n",
    "\n",
    "    # 3. CONSTRUIR O DECODER (CAMINHO DE EXPANSÃO)\n",
    "    # Vamos subir, aumentando a resolução e concatenando com as skip connections.\n",
    "    \n",
    "    # Bloco expansivo 1\n",
    "    # Sobe de 4x4 para 8x8\n",
    "    up_stack_1 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(encoder_final_output)\n",
    "    concat_1 = Concatenate()([up_stack_1, encoder_outputs[3]]) # Conecta com a saída do block4_pool\n",
    "    conv_stack_1 = Conv2D(128, 3, activation='relu', padding='same')(concat_1)\n",
    "    conv_stack_1 = Conv2D(128, 3, activation='relu', padding='same')(conv_stack_1)\n",
    "\n",
    "    # Bloco expansivo 2\n",
    "    # Sobe de 8x8 para 16x16\n",
    "    up_stack_2 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv_stack_1)\n",
    "    concat_2 = Concatenate()([up_stack_2, encoder_outputs[2]]) # Conecta com a saída do block3_pool\n",
    "    conv_stack_2 = Conv2D(64, 3, activation='relu', padding='same')(concat_2)\n",
    "    conv_stack_2 = Conv2D(64, 3, activation='relu', padding='same')(conv_stack_2)\n",
    "\n",
    "    # Bloco expansivo 3\n",
    "    # Sobe de 16x16 para 32x32\n",
    "    up_stack_3 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv_stack_2)\n",
    "    concat_3 = Concatenate()([up_stack_3, encoder_outputs[1]]) # Conecta com a saída do block2_pool\n",
    "    conv_stack_3 = Conv2D(32, 3, activation='relu', padding='same')(concat_3)\n",
    "    conv_stack_3 = Conv2D(32, 3, activation='relu', padding='same')(conv_stack_3)\n",
    "\n",
    "    # Bloco expansivo 4\n",
    "    # Sobe de 32x32 para 64x64\n",
    "    up_stack_4 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(conv_stack_3)\n",
    "    concat_4 = Concatenate()([up_stack_4, encoder_outputs[0]]) # Conecta com a saída do block1_pool\n",
    "    conv_stack_4 = Conv2D(16, 3, activation='relu', padding='same')(concat_4)\n",
    "    conv_stack_4 = Conv2D(16, 3, activation='relu', padding='same')(conv_stack_4)\n",
    "    \n",
    "    # Bloco expansivo 5 (final para restaurar o tamanho original)\n",
    "    # Sobe de 64x64 para 128x128\n",
    "    up_stack_5 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same')(conv_stack_4)\n",
    "    conv_stack_5 = Conv2D(8, 3, activation='relu', padding='same')(up_stack_5)\n",
    "    conv_stack_5 = Conv2D(8, 3, activation='relu', padding='same')(conv_stack_5)\n",
    "\n",
    "\n",
    "    # 4. CAMADA DE SAÍDA\n",
    "    # Usa um filtro com o número de classes e a ativação apropriada.\n",
    "    # Para segmentação binária, é 1 classe com ativação 'sigmoid'.\n",
    "    output_layer = Conv2D(num_classes, 1, activation='sigmoid')(conv_stack_5)\n",
    "\n",
    "    # 5. CRIAR E RETORNAR O MODELO FINAL\n",
    "    # A entrada do modelo é a entrada do backbone VGG16.\n",
    "    model = keras.Model(inputs=backbone.input, outputs=output_layer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- COMO USAR ---\n",
    "\n",
    "# 1. Construa o modelo\n",
    "input_shape = (128, 128, 3)\n",
    "model = build_unet_mobilenetv2(input_shape=input_shape)\n",
    "\n",
    "# 3. Veja o resultado\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred):\n",
    "    # Garantir que ambos os tensores sejam do tipo float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)  # Binarizar y_pred com threshold de 0.5\n",
    "    \n",
    "    # Calcular interseção e união\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    \n",
    "    # Prevenir divisão por zero\n",
    "    return tf.math.divide_no_nan(intersection, union)\n",
    "\n",
    "def dice_coef(y_true, y_pred, threshold=0.5, epsilon=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > threshold, tf.float32)\n",
    "    y_true_flat = keras.layers.Flatten()(y_true)\n",
    "    y_pred_flat = keras.layers.Flatten()(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true_flat * y_pred_flat)\n",
    "    return (2. * intersection + epsilon) / (tf.reduce_sum(y_true_flat) + tf.reduce_sum(y_pred_flat) + epsilon)\n",
    "\n",
    "def specificity(y_true, y_pred, threshold=0.5, epsilon=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > threshold, tf.float32)\n",
    "    tn = tf.reduce_sum((1 - y_true) * (1 - y_pred))\n",
    "    fp = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    return tn / (tn + fp + epsilon)\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    dice_coefficient = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - dice_coefficient # A perda é 1 - o coeficiente\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred, pos_weight=100.):\n",
    "    y_true = K.cast(y_true, tf.float32)\n",
    "    \n",
    "    # Calcula a cross-entropy\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Aplica os pesos\n",
    "    weight_vector = y_true * pos_weight + (1. - y_true)\n",
    "    weighted_bce = weight_vector * bce\n",
    "    \n",
    "    return K.mean(weighted_bce)\n",
    "\n",
    "def combined_loss(y_true, y_pred, alpha=0.3):\n",
    "    return alpha * weighted_binary_crossentropy(y_true, y_pred) + (1 - alpha) * dice_loss(y_true, y_pred)\n",
    "\n",
    "\n",
    "def train_model(train_image_dir, train_mask_dir, epochs, crop_size, batch_size, lr, gamma, experiment_name, run_name, model_name, saving_dir, augment):\n",
    "    # Carregar, recortar e aplicar data augmentation nos dados de treinamento\n",
    "    print(\"Carregando, recortando e aplicando data augmentation nos dados de treinamento...\")\n",
    "    start_time = time.time()\n",
    "    train_images, train_masks = load_and_crop_data(train_image_dir, train_mask_dir, crop_size=crop_size, augment=augment)\n",
    "    print(f\"Dados de treinamento carregados e aumentados em {time.time() - start_time:.2f} segundos\")\n",
    "    print(f\"Número total de imagens: {len(train_images)}\")\n",
    "\n",
    "    # Dividir os dados em 70% treino e 30% validação\n",
    "    train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        train_images, train_masks, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Criar datasets para treinamento e validação\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_masks))\n",
    "    train_dataset = train_dataset.shuffle(len(train_images)).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_masks))\n",
    "    val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    print(f\"Treinamento: {len(train_images)} imagens\")\n",
    "    print(f\"Validação: {len(val_images)} imagens\")\n",
    "\n",
    "    # Construir modelo\n",
    "    print(\"Construindo o modelo...\")\n",
    "    input_shape = (crop_size, crop_size, 3)  # Imagens recortadas\n",
    "    model = build_unet_mobilenetv2(input_shape)\n",
    "    print(\"Modelo construído!\")\n",
    "\n",
    "    mlflow.set_tracking_uri('http://localhost:5000')\n",
    "\n",
    "    # Defina um nome para o seu experimento no MLflow\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # >>>>> A MÁGICA ACONTECE AQUI <<<<<\n",
    "    # Ative o autologging para Keras\n",
    "    mlflow.keras.autolog()\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "        # Defina as métricas que o Keras irá calcular (e o MLflow irá capturar)\n",
    "        metrics_to_track = [\n",
    "            keras.metrics.BinaryIoU(threshold=0.5, name='iou'),\n",
    "            keras.metrics.BinaryAccuracy(threshold=0.5, name='accuracy'),\n",
    "            keras.metrics.Precision(thresholds=0.5, name='precision'),\n",
    "            keras.metrics.Recall(thresholds=0.5, name='recall'),\n",
    "            dice_coef,\n",
    "            specificity\n",
    "        ]\n",
    "\n",
    "        # Compilar modelo com Focal Loss\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), \n",
    "                    loss=combined_loss, \n",
    "                    metrics=metrics_to_track\n",
    "                    )\n",
    "\n",
    "        # Configurar callback para ajuste do learning rate\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            monitor='val_iou',   # Monitorar o iou no conjunto de validação\n",
    "            factor=0.5,          # Fator de redução da taxa de aprendizado\n",
    "            patience=5,          # Número de épocas sem melhora antes de reduzir\n",
    "            min_lr=1e-6,         # Limite inferior para o learning rate\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Configurar callback para salvar modelo\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            saving_dir+'/'+f'{model_name}.keras',\n",
    "            monitor='val_iou',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "\n",
    "        )\n",
    "\n",
    "        # Configurar callback para early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_iou',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "\n",
    "        )\n",
    "\n",
    "        # Treinar modelo\n",
    "        print(\"Iniciando o treinamento...\")\n",
    "        print(\"Iniciando run do MLflow:\", run.info.run_id)\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.fit(\n",
    "            train_dataset, \n",
    "            validation_data=val_dataset,  # Adiciona o conjunto de validação\n",
    "            epochs=epochs, \n",
    "            verbose=1,\n",
    "            callbacks=[lr_scheduler, model_checkpoint, early_stopping]\n",
    "        )\n",
    "\n",
    "    print(f\"Treinamento concluído em {time.time() - start_time:.2f} segundos\")\n",
    "    print(\"Carregando o melhor modelo salvo do arquivo 'melhor_modelo.keras'...\")\n",
    "\n",
    "    # Não se esqueça de passar seus objetos customizados para que o Keras os reconheça!\n",
    "    custom_objects = {\n",
    "        'combined_loss': combined_loss, \n",
    "        'dice_loss': dice_loss, \n",
    "        'weighted_binary_crossentropy': weighted_binary_crossentropy,\n",
    "        'iou': iou,\n",
    "        'dice_coef': dice_coef,\n",
    "        'specificity': specificity\n",
    "    }\n",
    "\n",
    "    # Carrega o melhor modelo que foi salvo durante o treinamento\n",
    "    best_model = load_model(saving_dir+'/'+f'{model_name}.keras', custom_objects=custom_objects)\n",
    "\n",
    "    print(f\"Salvando o modelo no formato de pasta (TensorFlow SavedModel) em '{saving_dir}'...\")\n",
    "\n",
    "    # Salva o modelo novamente, mas desta vez sem extensão, para criar a pasta\n",
    "    best_model.export(saving_dir+'/'+f'{model_name}')\n",
    "\n",
    "    print(\"Processo concluído com sucesso!\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificação dos dados de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição completa da classe DataGenerator\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_dir, mask_dir, crop_size, batch_size, augment=False):\n",
    "        self.image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir)])\n",
    "        self.mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir)])\n",
    "        self.crop_size = crop_size\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augment\n",
    "        \n",
    "        self.all_patches = self._create_patch_list()\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def _create_patch_list(self):\n",
    "        patch_coords = []\n",
    "        for i, img_path in enumerate(self.image_files):\n",
    "            try:\n",
    "                img_ds = gdal.Open(img_path)\n",
    "                if img_ds is None:\n",
    "                    print(f\"Aviso: Não foi possível abrir a imagem {img_path}. Pulando.\")\n",
    "                    continue\n",
    "                img_width, img_height = img_ds.RasterXSize, img_ds.RasterYSize\n",
    "                \n",
    "                for y in range(0, img_height - self.crop_size + 1, self.crop_size):\n",
    "                    for x in range(0, img_width - self.crop_size + 1, self.crop_size):\n",
    "                        patch_coords.append({'file_index': i, 'x': x, 'y': y})\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar o arquivo {img_path}: {e}\")\n",
    "        return patch_coords\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.all_patches) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_patch_indices = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        patches_to_load = [self.all_patches[i] for i in batch_patch_indices]\n",
    "        X, y = self._data_generation(patches_to_load)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.all_patches))\n",
    "        np.random.shuffle(self.indexes)\n",
    "\n",
    "    def _data_generation(self, patches_to_load):\n",
    "        X = np.empty((self.batch_size, self.crop_size, self.crop_size, 3), dtype=np.float32)\n",
    "        y = np.empty((self.batch_size, self.crop_size, self.crop_size, 1), dtype=np.float32)\n",
    "\n",
    "        for i, patch_info in enumerate(patches_to_load):\n",
    "            file_idx = patch_info['file_index']\n",
    "            x, y_coord = patch_info['x'], patch_info['y']\n",
    "            \n",
    "            img_ds = gdal.Open(self.image_files[file_idx])\n",
    "            mask_ds = gdal.Open(self.mask_files[file_idx])\n",
    "\n",
    "            img_block = img_ds.ReadAsArray(x, y_coord, self.crop_size, self.crop_size)\n",
    "            mask_block = mask_ds.ReadAsArray(x, y_coord, self.crop_size, self.crop_size)\n",
    "\n",
    "            img_block = np.transpose(img_block, (1, 2, 0))\n",
    "            if img_block.shape[2] == 4:\n",
    "                img_block = img_block[:, :, :3]\n",
    "            \n",
    "            if mask_block.ndim == 2:\n",
    "                mask_block = np.expand_dims(mask_block, axis=-1)\n",
    "\n",
    "            img_block = (img_block / 255.0).astype(np.float32)\n",
    "            mask_block = (mask_block).astype(np.float32)\n",
    "            \n",
    "            if self.augment:\n",
    "                img_block, mask_block = augment_data(img_block, mask_block)\n",
    "\n",
    "            X[i,] = img_block\n",
    "            y[i,] = mask_block\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- Configure os seus parâmetros aqui ---\n",
    "# (Use os mesmos valores que você usa para o treinamento)\n",
    "crop_size = 128\n",
    "batch_size = 8 # Pode ser qualquer valor, ex: 8, para visualização\n",
    "augment = False # Mantenha como False para uma verificação limpa dos dados originais\n",
    "\n",
    "# --- Fim da Configuração ---\n",
    "\n",
    "\n",
    "# 1. Crie uma instância do seu DataGenerator\n",
    "# (Assumindo que a classe DataGenerator que definimos anteriormente está disponível)\n",
    "try:\n",
    "    train_generator = DataGenerator(\n",
    "        image_dir=train_image_dir,\n",
    "        mask_dir=train_mask_dir,\n",
    "        crop_size=crop_size,\n",
    "        batch_size=batch_size,\n",
    "        augment=augment\n",
    "    )\n",
    "except NameError:\n",
    "    print(\"ERRO: A classe 'DataGenerator' não foi encontrada. Certifique-se de que ela está definida no seu script.\")\n",
    "    # Se der este erro, pare e cole a definição da classe DataGenerator aqui.\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO: Verifique se os caminhos '{train_image_dir}' e '{train_mask_dir}' estão corretos.\")\n",
    "    # Se der este erro, corrija os caminhos na seção de configuração acima.\n",
    "\n",
    "\n",
    "# 2. Pega o primeiro lote de dados gerado\n",
    "print(\"Gerando o primeiro lote de dados para visualização...\")\n",
    "images, masks = train_generator[0] \n",
    "\n",
    "# 3. Imprime informações de diagnóstico (Sanity Check)\n",
    "print(f\"Forma (shape) do lote de imagens: {images.shape}\") # Deve ser (batch_size, 128, 128, 3)\n",
    "print(f\"Forma (shape) do lote de máscaras: {masks.shape}\") # Deve ser (batch_size, 128, 128, 1)\n",
    "print(f\"Tipo de dados das imagens: {images.dtype}\") # Deve ser float32\n",
    "print(f\"Tipo de dados das máscaras: {masks.dtype}\") # Deve ser float32\n",
    "print(f\"Valores únicos na primeira máscara (deveria ser 0 e 1): {np.unique(masks[0])}\")\n",
    "\n",
    "\n",
    "# 4. Plota as imagens e máscaras\n",
    "print(\"\\nPlotando as imagens e máscaras...\")\n",
    "n_samples = min(5, batch_size)  # Mostra até 5 amostras do lote\n",
    "\n",
    "plt.figure(figsize=(10, n_samples * 3))\n",
    "for i in range(n_samples):\n",
    "    # Plota a imagem original\n",
    "    plt.subplot(n_samples, 2, 2 * i + 1)\n",
    "    # A imagem foi normalizada (dividida por 255), então ela pode parecer escura ou estranha, isso é normal.\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(f\"Imagem de Treino {i}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plota a máscara correspondente\n",
    "    plt.subplot(n_samples, 2, 2 * i + 2)\n",
    "    # Usamos .squeeze() para remover a dimensão do canal (128, 128, 1) -> (128, 128)\n",
    "    # Usamos cmap='gray' para garantir a visualização em preto e branco\n",
    "    plt.imshow(masks[i].squeeze(), cmap='gray')\n",
    "    plt.title(f\"Máscara de Treino {i}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criar experimento no MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri='http://127.0.0.1:5000')\n",
    "\n",
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"Projeto de detecção de crimes transfronteiriços na Amazônia. \"\n",
    "    \"Este experimento contém os modelos de detecção de áreas com indícios de pistas de pouso ilegais\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"nome_projeto\":\"Identificar Crimes Transfronteiriços na Amazônia\",\n",
    "    \"tipo_crime\": \"Pistas de pouso\",\n",
    "    \"mlflow.note.content\": experiment_description\n",
    "\n",
    "}\n",
    "\n",
    "# Create the Experiment, providing a unique name\n",
    "garimpos_experiment = client.create_experiment(\n",
    "    name=\"Modelos_pistas\", tags=experiment_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando, recortando e aplicando data augmentation nos dados de treinamento...\n",
      "Dados de treinamento carregados e aumentados em 20.18 segundos\n",
      "Número total de imagens: 15970\n",
      "Treinamento: 11179 imagens\n",
      "Validação: 4791 imagens\n",
      "Construindo o modelo...\n",
      "Modelo construído!\n",
      "Iniciando o treinamento...\n",
      "Iniciando run do MLflow: 892c93460ef246e495c99fd6aa449d01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7418 - dice_coef: 0.3594 - iou: 0.4664 - loss: 0.8804 - precision: 0.2086 - recall: 0.9801 - specificity: 0.7242\n",
      "Epoch 1: val_iou improved from -inf to 0.63783, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 83ms/step - accuracy: 0.7418 - dice_coef: 0.3595 - iou: 0.4665 - loss: 0.8803 - precision: 0.2087 - recall: 0.9801 - specificity: 0.7242 - val_accuracy: 0.9042 - val_dice_coef: 0.4794 - val_iou: 0.6378 - val_loss: 0.5916 - val_precision: 0.3800 - val_recall: 0.9817 - val_specificity: 0.8979 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8824 - dice_coef: 0.4569 - iou: 0.6072 - loss: 0.5848 - precision: 0.3409 - recall: 0.9893 - specificity: 0.8733\n",
      "Epoch 2: val_iou did not improve from 0.63783\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 83ms/step - accuracy: 0.8824 - dice_coef: 0.4569 - iou: 0.6072 - loss: 0.5848 - precision: 0.3409 - recall: 0.9893 - specificity: 0.8733 - val_accuracy: 0.8900 - val_dice_coef: 0.4467 - val_iou: 0.6147 - val_loss: 0.6188 - val_precision: 0.3480 - val_recall: 0.9861 - val_specificity: 0.8822 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8966 - dice_coef: 0.4811 - iou: 0.6266 - loss: 0.6086 - precision: 0.3652 - recall: 0.9838 - specificity: 0.8894\n",
      "Epoch 3: val_iou did not improve from 0.63783\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 80ms/step - accuracy: 0.8966 - dice_coef: 0.4811 - iou: 0.6266 - loss: 0.6085 - precision: 0.3652 - recall: 0.9838 - specificity: 0.8894 - val_accuracy: 0.8717 - val_dice_coef: 0.4203 - val_iou: 0.5887 - val_loss: 0.5865 - val_precision: 0.3145 - val_recall: 0.9913 - val_specificity: 0.8623 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9156 - dice_coef: 0.5216 - iou: 0.6585 - loss: 0.5608 - precision: 0.4094 - recall: 0.9832 - specificity: 0.9099\n",
      "Epoch 4: val_iou improved from 0.63783 to 0.68889, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 81ms/step - accuracy: 0.9156 - dice_coef: 0.5216 - iou: 0.6585 - loss: 0.5608 - precision: 0.4094 - recall: 0.9832 - specificity: 0.9099 - val_accuracy: 0.9300 - val_dice_coef: 0.5498 - val_iou: 0.6889 - val_loss: 0.5328 - val_precision: 0.4568 - val_recall: 0.9777 - val_specificity: 0.9257 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9124 - dice_coef: 0.5248 - iou: 0.6548 - loss: 0.5407 - precision: 0.4051 - recall: 0.9857 - specificity: 0.9062\n",
      "Epoch 5: val_iou did not improve from 0.68889\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 84ms/step - accuracy: 0.9124 - dice_coef: 0.5248 - iou: 0.6548 - loss: 0.5407 - precision: 0.4051 - recall: 0.9857 - specificity: 0.9062 - val_accuracy: 0.9290 - val_dice_coef: 0.5465 - val_iou: 0.6872 - val_loss: 0.5221 - val_precision: 0.4536 - val_recall: 0.9815 - val_specificity: 0.9245 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9253 - dice_coef: 0.5491 - iou: 0.6800 - loss: 0.4948 - precision: 0.4421 - recall: 0.9856 - specificity: 0.9202\n",
      "Epoch 6: val_iou improved from 0.68889 to 0.71785, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 74ms/step - accuracy: 0.9253 - dice_coef: 0.5491 - iou: 0.6800 - loss: 0.4948 - precision: 0.4421 - recall: 0.9856 - specificity: 0.9202 - val_accuracy: 0.9419 - val_dice_coef: 0.5847 - val_iou: 0.7179 - val_loss: 0.5383 - val_precision: 0.5046 - val_recall: 0.9718 - val_specificity: 0.9391 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9315 - dice_coef: 0.5579 - iou: 0.6903 - loss: 0.4955 - precision: 0.4561 - recall: 0.9857 - specificity: 0.9270\n",
      "Epoch 7: val_iou did not improve from 0.71785\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 75ms/step - accuracy: 0.9315 - dice_coef: 0.5579 - iou: 0.6903 - loss: 0.4955 - precision: 0.4561 - recall: 0.9857 - specificity: 0.9270 - val_accuracy: 0.9225 - val_dice_coef: 0.5283 - val_iou: 0.6736 - val_loss: 0.5023 - val_precision: 0.4319 - val_recall: 0.9870 - val_specificity: 0.9171 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9297 - dice_coef: 0.5558 - iou: 0.6903 - loss: 0.4923 - precision: 0.4581 - recall: 0.9864 - specificity: 0.9249\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 8: val_iou improved from 0.71785 to 0.73749, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 77ms/step - accuracy: 0.9297 - dice_coef: 0.5558 - iou: 0.6903 - loss: 0.4923 - precision: 0.4581 - recall: 0.9864 - specificity: 0.9249 - val_accuracy: 0.9488 - val_dice_coef: 0.6115 - val_iou: 0.7375 - val_loss: 0.5101 - val_precision: 0.5370 - val_recall: 0.9736 - val_specificity: 0.9463 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9404 - dice_coef: 0.6043 - iou: 0.7173 - loss: 0.4640 - precision: 0.5018 - recall: 0.9845 - specificity: 0.9367\n",
      "Epoch 9: val_iou did not improve from 0.73749\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 83ms/step - accuracy: 0.9404 - dice_coef: 0.6043 - iou: 0.7173 - loss: 0.4640 - precision: 0.5018 - recall: 0.9845 - specificity: 0.9367 - val_accuracy: 0.9221 - val_dice_coef: 0.5276 - val_iou: 0.6731 - val_loss: 0.4998 - val_precision: 0.4310 - val_recall: 0.9887 - val_specificity: 0.9166 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9423 - dice_coef: 0.5951 - iou: 0.7178 - loss: 0.4681 - precision: 0.5012 - recall: 0.9827 - specificity: 0.9389\n",
      "Epoch 10: val_iou did not improve from 0.73749\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 84ms/step - accuracy: 0.9423 - dice_coef: 0.5952 - iou: 0.7178 - loss: 0.4680 - precision: 0.5012 - recall: 0.9827 - specificity: 0.9389 - val_accuracy: 0.9473 - val_dice_coef: 0.6058 - val_iou: 0.7334 - val_loss: 0.4763 - val_precision: 0.5292 - val_recall: 0.9775 - val_specificity: 0.9444 - learning_rate: 5.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9476 - dice_coef: 0.6168 - iou: 0.7338 - loss: 0.4173 - precision: 0.5270 - recall: 0.9863 - specificity: 0.9442\n",
      "Epoch 11: val_iou improved from 0.73749 to 0.75475, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 83ms/step - accuracy: 0.9476 - dice_coef: 0.6168 - iou: 0.7338 - loss: 0.4173 - precision: 0.5270 - recall: 0.9863 - specificity: 0.9442 - val_accuracy: 0.9545 - val_dice_coef: 0.6349 - val_iou: 0.7548 - val_loss: 0.4724 - val_precision: 0.5670 - val_recall: 0.9718 - val_specificity: 0.9526 - learning_rate: 5.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9468 - dice_coef: 0.6194 - iou: 0.7329 - loss: 0.4452 - precision: 0.5271 - recall: 0.9826 - specificity: 0.9436\n",
      "Epoch 12: val_iou did not improve from 0.75475\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 84ms/step - accuracy: 0.9469 - dice_coef: 0.6194 - iou: 0.7329 - loss: 0.4452 - precision: 0.5271 - recall: 0.9826 - specificity: 0.9436 - val_accuracy: 0.9468 - val_dice_coef: 0.6009 - val_iou: 0.7319 - val_loss: 0.4836 - val_precision: 0.5269 - val_recall: 0.9763 - val_specificity: 0.9440 - learning_rate: 5.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9525 - dice_coef: 0.6415 - iou: 0.7508 - loss: 0.4174 - precision: 0.5573 - recall: 0.9834 - specificity: 0.9496\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 13: val_iou did not improve from 0.75475\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 82ms/step - accuracy: 0.9525 - dice_coef: 0.6415 - iou: 0.7508 - loss: 0.4174 - precision: 0.5573 - recall: 0.9834 - specificity: 0.9496 - val_accuracy: 0.9530 - val_dice_coef: 0.6282 - val_iou: 0.7501 - val_loss: 0.4939 - val_precision: 0.5587 - val_recall: 0.9726 - val_specificity: 0.9509 - learning_rate: 5.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9571 - dice_coef: 0.6525 - iou: 0.7633 - loss: 0.3887 - precision: 0.5771 - recall: 0.9854 - specificity: 0.9545\n",
      "Epoch 14: val_iou improved from 0.75475 to 0.77424, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 84ms/step - accuracy: 0.9571 - dice_coef: 0.6525 - iou: 0.7633 - loss: 0.3887 - precision: 0.5771 - recall: 0.9854 - specificity: 0.9545 - val_accuracy: 0.9605 - val_dice_coef: 0.6628 - val_iou: 0.7742 - val_loss: 0.5141 - val_precision: 0.6040 - val_recall: 0.9633 - val_specificity: 0.9596 - learning_rate: 2.5000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9564 - dice_coef: 0.6513 - iou: 0.7639 - loss: 0.3831 - precision: 0.5789 - recall: 0.9859 - specificity: 0.9536\n",
      "Epoch 15: val_iou did not improve from 0.77424\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 90ms/step - accuracy: 0.9564 - dice_coef: 0.6513 - iou: 0.7639 - loss: 0.3831 - precision: 0.5789 - recall: 0.9859 - specificity: 0.9536 - val_accuracy: 0.9569 - val_dice_coef: 0.6452 - val_iou: 0.7623 - val_loss: 0.5038 - val_precision: 0.5813 - val_recall: 0.9680 - val_specificity: 0.9553 - learning_rate: 2.5000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9583 - dice_coef: 0.6590 - iou: 0.7702 - loss: 0.3660 - precision: 0.5890 - recall: 0.9875 - specificity: 0.9556\n",
      "Epoch 16: val_iou did not improve from 0.77424\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 86ms/step - accuracy: 0.9583 - dice_coef: 0.6590 - iou: 0.7702 - loss: 0.3660 - precision: 0.5890 - recall: 0.9875 - specificity: 0.9556 - val_accuracy: 0.9581 - val_dice_coef: 0.6524 - val_iou: 0.7663 - val_loss: 0.5138 - val_precision: 0.5889 - val_recall: 0.9665 - val_specificity: 0.9568 - learning_rate: 2.5000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9577 - dice_coef: 0.6636 - iou: 0.7679 - loss: 0.3727 - precision: 0.5856 - recall: 0.9860 - specificity: 0.9549\n",
      "Epoch 17: val_iou did not improve from 0.77424\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 87ms/step - accuracy: 0.9577 - dice_coef: 0.6636 - iou: 0.7679 - loss: 0.3727 - precision: 0.5856 - recall: 0.9860 - specificity: 0.9549 - val_accuracy: 0.9557 - val_dice_coef: 0.6409 - val_iou: 0.7587 - val_loss: 0.4950 - val_precision: 0.5742 - val_recall: 0.9707 - val_specificity: 0.9539 - learning_rate: 2.5000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9617 - dice_coef: 0.6713 - iou: 0.7819 - loss: 0.3510 - precision: 0.6090 - recall: 0.9881 - specificity: 0.9591\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 18: val_iou did not improve from 0.77424\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 93ms/step - accuracy: 0.9617 - dice_coef: 0.6713 - iou: 0.7819 - loss: 0.3510 - precision: 0.6089 - recall: 0.9881 - specificity: 0.9591 - val_accuracy: 0.9390 - val_dice_coef: 0.5774 - val_iou: 0.7112 - val_loss: 0.4921 - val_precision: 0.4920 - val_recall: 0.9798 - val_specificity: 0.9352 - learning_rate: 2.5000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9574 - dice_coef: 0.6649 - iou: 0.7690 - loss: 0.3560 - precision: 0.5872 - recall: 0.9889 - specificity: 0.9547\n",
      "Epoch 19: val_iou did not improve from 0.77424\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 101ms/step - accuracy: 0.9574 - dice_coef: 0.6649 - iou: 0.7690 - loss: 0.3560 - precision: 0.5873 - recall: 0.9889 - specificity: 0.9547 - val_accuracy: 0.9586 - val_dice_coef: 0.6539 - val_iou: 0.7683 - val_loss: 0.4879 - val_precision: 0.5913 - val_recall: 0.9699 - val_specificity: 0.9571 - learning_rate: 1.2500e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9602 - dice_coef: 0.6733 - iou: 0.7805 - loss: 0.3379 - precision: 0.6070 - recall: 0.9902 - specificity: 0.9575\n",
      "Epoch 20: val_iou improved from 0.77424 to 0.77763, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 103ms/step - accuracy: 0.9602 - dice_coef: 0.6734 - iou: 0.7805 - loss: 0.3380 - precision: 0.6070 - recall: 0.9902 - specificity: 0.9575 - val_accuracy: 0.9616 - val_dice_coef: 0.6693 - val_iou: 0.7776 - val_loss: 0.5630 - val_precision: 0.6119 - val_recall: 0.9582 - val_specificity: 0.9610 - learning_rate: 1.2500e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9628 - dice_coef: 0.6942 - iou: 0.7834 - loss: 0.3312 - precision: 0.6103 - recall: 0.9889 - specificity: 0.9605\n",
      "Epoch 21: val_iou improved from 0.77763 to 0.78078, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 108ms/step - accuracy: 0.9628 - dice_coef: 0.6941 - iou: 0.7834 - loss: 0.3312 - precision: 0.6103 - recall: 0.9889 - specificity: 0.9605 - val_accuracy: 0.9623 - val_dice_coef: 0.6721 - val_iou: 0.7808 - val_loss: 0.5520 - val_precision: 0.6163 - val_recall: 0.9616 - val_specificity: 0.9616 - learning_rate: 1.2500e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9630 - dice_coef: 0.6868 - iou: 0.7886 - loss: 0.3278 - precision: 0.6205 - recall: 0.9898 - specificity: 0.9605\n",
      "Epoch 22: val_iou did not improve from 0.78078\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 108ms/step - accuracy: 0.9630 - dice_coef: 0.6868 - iou: 0.7886 - loss: 0.3278 - precision: 0.6205 - recall: 0.9898 - specificity: 0.9605 - val_accuracy: 0.9603 - val_dice_coef: 0.6624 - val_iou: 0.7736 - val_loss: 0.5493 - val_precision: 0.6023 - val_recall: 0.9649 - val_specificity: 0.9592 - learning_rate: 1.2500e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9642 - dice_coef: 0.6906 - iou: 0.7901 - loss: 0.3199 - precision: 0.6220 - recall: 0.9903 - specificity: 0.9619\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 23: val_iou did not improve from 0.78078\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 109ms/step - accuracy: 0.9642 - dice_coef: 0.6906 - iou: 0.7901 - loss: 0.3199 - precision: 0.6220 - recall: 0.9903 - specificity: 0.9619 - val_accuracy: 0.9601 - val_dice_coef: 0.6620 - val_iou: 0.7733 - val_loss: 0.5598 - val_precision: 0.6015 - val_recall: 0.9658 - val_specificity: 0.9590 - learning_rate: 1.2500e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9662 - dice_coef: 0.7005 - iou: 0.7959 - loss: 0.3120 - precision: 0.6315 - recall: 0.9902 - specificity: 0.9640\n",
      "Epoch 24: val_iou improved from 0.78078 to 0.78230, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 112ms/step - accuracy: 0.9662 - dice_coef: 0.7005 - iou: 0.7959 - loss: 0.3120 - precision: 0.6315 - recall: 0.9902 - specificity: 0.9640 - val_accuracy: 0.9627 - val_dice_coef: 0.6742 - val_iou: 0.7823 - val_loss: 0.5705 - val_precision: 0.6190 - val_recall: 0.9617 - val_specificity: 0.9621 - learning_rate: 6.2500e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9653 - dice_coef: 0.7026 - iou: 0.7977 - loss: 0.3078 - precision: 0.6359 - recall: 0.9911 - specificity: 0.9630\n",
      "Epoch 25: val_iou did not improve from 0.78230\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 111ms/step - accuracy: 0.9653 - dice_coef: 0.7026 - iou: 0.7977 - loss: 0.3078 - precision: 0.6359 - recall: 0.9911 - specificity: 0.9630 - val_accuracy: 0.9591 - val_dice_coef: 0.6575 - val_iou: 0.7698 - val_loss: 0.5365 - val_precision: 0.5945 - val_recall: 0.9680 - val_specificity: 0.9577 - learning_rate: 6.2500e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9644 - dice_coef: 0.6865 - iou: 0.7934 - loss: 0.3178 - precision: 0.6280 - recall: 0.9917 - specificity: 0.9618\n",
      "Epoch 26: val_iou improved from 0.78230 to 0.78471, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 116ms/step - accuracy: 0.9644 - dice_coef: 0.6865 - iou: 0.7934 - loss: 0.3177 - precision: 0.6280 - recall: 0.9917 - specificity: 0.9618 - val_accuracy: 0.9635 - val_dice_coef: 0.6779 - val_iou: 0.7847 - val_loss: 0.6160 - val_precision: 0.6246 - val_recall: 0.9585 - val_specificity: 0.9631 - learning_rate: 6.2500e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9651 - dice_coef: 0.6998 - iou: 0.7975 - loss: 0.3101 - precision: 0.6356 - recall: 0.9912 - specificity: 0.9626\n",
      "Epoch 27: val_iou did not improve from 0.78471\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 138ms/step - accuracy: 0.9651 - dice_coef: 0.6998 - iou: 0.7975 - loss: 0.3101 - precision: 0.6356 - recall: 0.9912 - specificity: 0.9626 - val_accuracy: 0.9627 - val_dice_coef: 0.6742 - val_iou: 0.7824 - val_loss: 0.5733 - val_precision: 0.6190 - val_recall: 0.9620 - val_specificity: 0.9620 - learning_rate: 6.2500e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m1397/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9661 - dice_coef: 0.7034 - iou: 0.7985 - loss: 0.3014 - precision: 0.6365 - recall: 0.9914 - specificity: 0.9637\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 28: val_iou did not improve from 0.78471\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 102ms/step - accuracy: 0.9661 - dice_coef: 0.7034 - iou: 0.7985 - loss: 0.3014 - precision: 0.6365 - recall: 0.9914 - specificity: 0.9637 - val_accuracy: 0.9611 - val_dice_coef: 0.6670 - val_iou: 0.7764 - val_loss: 0.5932 - val_precision: 0.6082 - val_recall: 0.9624 - val_specificity: 0.9602 - learning_rate: 6.2500e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9662 - dice_coef: 0.7145 - iou: 0.8026 - loss: 0.2946 - precision: 0.6444 - recall: 0.9921 - specificity: 0.9638\n",
      "Epoch 29: val_iou improved from 0.78471 to 0.78747, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 118ms/step - accuracy: 0.9662 - dice_coef: 0.7145 - iou: 0.8026 - loss: 0.2946 - precision: 0.6444 - recall: 0.9921 - specificity: 0.9638 - val_accuracy: 0.9642 - val_dice_coef: 0.6809 - val_iou: 0.7875 - val_loss: 0.6332 - val_precision: 0.6300 - val_recall: 0.9574 - val_specificity: 0.9639 - learning_rate: 3.1250e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9671 - dice_coef: 0.7126 - iou: 0.8044 - loss: 0.2963 - precision: 0.6470 - recall: 0.9922 - specificity: 0.9649\n",
      "Epoch 30: val_iou improved from 0.78747 to 0.78861, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 133ms/step - accuracy: 0.9671 - dice_coef: 0.7126 - iou: 0.8044 - loss: 0.2963 - precision: 0.6470 - recall: 0.9922 - specificity: 0.9649 - val_accuracy: 0.9646 - val_dice_coef: 0.6826 - val_iou: 0.7886 - val_loss: 0.6537 - val_precision: 0.6325 - val_recall: 0.9564 - val_specificity: 0.9644 - learning_rate: 3.1250e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9677 - dice_coef: 0.7028 - iou: 0.8038 - loss: 0.3032 - precision: 0.6452 - recall: 0.9914 - specificity: 0.9655\n",
      "Epoch 31: val_iou did not improve from 0.78861\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 137ms/step - accuracy: 0.9677 - dice_coef: 0.7028 - iou: 0.8038 - loss: 0.3032 - precision: 0.6452 - recall: 0.9914 - specificity: 0.9655 - val_accuracy: 0.9629 - val_dice_coef: 0.6741 - val_iou: 0.7825 - val_loss: 0.6308 - val_precision: 0.6201 - val_recall: 0.9598 - val_specificity: 0.9623 - learning_rate: 3.1250e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.9683 - dice_coef: 0.7250 - iou: 0.8054 - loss: 0.2838 - precision: 0.6477 - recall: 0.9922 - specificity: 0.9662\n",
      "Epoch 32: val_iou did not improve from 0.78861\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 279ms/step - accuracy: 0.9683 - dice_coef: 0.7250 - iou: 0.8054 - loss: 0.2838 - precision: 0.6478 - recall: 0.9922 - specificity: 0.9662 - val_accuracy: 0.9632 - val_dice_coef: 0.6757 - val_iou: 0.7836 - val_loss: 0.6433 - val_precision: 0.6224 - val_recall: 0.9589 - val_specificity: 0.9627 - learning_rate: 3.1250e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.9684 - dice_coef: 0.7113 - iou: 0.8063 - loss: 0.2921 - precision: 0.6495 - recall: 0.9922 - specificity: 0.9662\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 33: val_iou improved from 0.78861 to 0.78980, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m527s\u001b[0m 377ms/step - accuracy: 0.9684 - dice_coef: 0.7113 - iou: 0.8063 - loss: 0.2921 - precision: 0.6495 - recall: 0.9922 - specificity: 0.9662 - val_accuracy: 0.9649 - val_dice_coef: 0.6838 - val_iou: 0.7898 - val_loss: 0.6783 - val_precision: 0.6355 - val_recall: 0.9544 - val_specificity: 0.9649 - learning_rate: 3.1250e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.9691 - dice_coef: 0.7277 - iou: 0.8104 - loss: 0.2822 - precision: 0.6571 - recall: 0.9920 - specificity: 0.9670\n",
      "Epoch 34: val_iou did not improve from 0.78980\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m569s\u001b[0m 407ms/step - accuracy: 0.9691 - dice_coef: 0.7277 - iou: 0.8104 - loss: 0.2822 - precision: 0.6571 - recall: 0.9920 - specificity: 0.9670 - val_accuracy: 0.9637 - val_dice_coef: 0.6784 - val_iou: 0.7855 - val_loss: 0.6597 - val_precision: 0.6263 - val_recall: 0.9578 - val_specificity: 0.9634 - learning_rate: 1.5625e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.9683 - dice_coef: 0.7099 - iou: 0.8058 - loss: 0.2944 - precision: 0.6484 - recall: 0.9924 - specificity: 0.9660\n",
      "Epoch 35: val_iou improved from 0.78980 to 0.79624, saving model to C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais.keras\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m601s\u001b[0m 430ms/step - accuracy: 0.9683 - dice_coef: 0.7099 - iou: 0.8058 - loss: 0.2944 - precision: 0.6484 - recall: 0.9924 - specificity: 0.9660 - val_accuracy: 0.9667 - val_dice_coef: 0.6925 - val_iou: 0.7962 - val_loss: 0.7179 - val_precision: 0.6489 - val_recall: 0.9508 - val_specificity: 0.9670 - learning_rate: 1.5625e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.9691 - dice_coef: 0.7183 - iou: 0.8121 - loss: 0.2871 - precision: 0.6605 - recall: 0.9923 - specificity: 0.9670\n",
      "Epoch 36: val_iou did not improve from 0.79624\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 403ms/step - accuracy: 0.9691 - dice_coef: 0.7183 - iou: 0.8121 - loss: 0.2871 - precision: 0.6604 - recall: 0.9923 - specificity: 0.9670 - val_accuracy: 0.9648 - val_dice_coef: 0.6837 - val_iou: 0.7892 - val_loss: 0.6949 - val_precision: 0.6344 - val_recall: 0.9544 - val_specificity: 0.9647 - learning_rate: 1.5625e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 0.9681 - dice_coef: 0.7226 - iou: 0.8099 - loss: 0.2859 - precision: 0.6569 - recall: 0.9927 - specificity: 0.9657\n",
      "Epoch 37: val_iou did not improve from 0.79624\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m509s\u001b[0m 364ms/step - accuracy: 0.9681 - dice_coef: 0.7226 - iou: 0.8099 - loss: 0.2859 - precision: 0.6569 - recall: 0.9927 - specificity: 0.9657 - val_accuracy: 0.9650 - val_dice_coef: 0.6843 - val_iou: 0.7899 - val_loss: 0.6883 - val_precision: 0.6357 - val_recall: 0.9545 - val_specificity: 0.9649 - learning_rate: 1.5625e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.9679 - dice_coef: 0.7218 - iou: 0.8066 - loss: 0.2840 - precision: 0.6504 - recall: 0.9928 - specificity: 0.9656\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 38: val_iou did not improve from 0.79624\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 396ms/step - accuracy: 0.9679 - dice_coef: 0.7218 - iou: 0.8066 - loss: 0.2840 - precision: 0.6504 - recall: 0.9928 - specificity: 0.9656 - val_accuracy: 0.9653 - val_dice_coef: 0.6861 - val_iou: 0.7912 - val_loss: 0.7061 - val_precision: 0.6385 - val_recall: 0.9532 - val_specificity: 0.9654 - learning_rate: 1.5625e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step - accuracy: 0.9693 - dice_coef: 0.7110 - iou: 0.8108 - loss: 0.2897 - precision: 0.6574 - recall: 0.9925 - specificity: 0.9670\n",
      "Epoch 39: val_iou did not improve from 0.79624\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 436ms/step - accuracy: 0.9693 - dice_coef: 0.7110 - iou: 0.8108 - loss: 0.2897 - precision: 0.6574 - recall: 0.9925 - specificity: 0.9670 - val_accuracy: 0.9654 - val_dice_coef: 0.6867 - val_iou: 0.7916 - val_loss: 0.7076 - val_precision: 0.6394 - val_recall: 0.9530 - val_specificity: 0.9655 - learning_rate: 7.8125e-06\n",
      "Epoch 40/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.9689 - dice_coef: 0.7323 - iou: 0.8140 - loss: 0.2775 - precision: 0.6636 - recall: 0.9922 - specificity: 0.9667\n",
      "Epoch 40: val_iou did not improve from 0.79624\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 442ms/step - accuracy: 0.9689 - dice_coef: 0.7323 - iou: 0.8140 - loss: 0.2775 - precision: 0.6636 - recall: 0.9922 - specificity: 0.9667 - val_accuracy: 0.9650 - val_dice_coef: 0.6848 - val_iou: 0.7901 - val_loss: 0.7015 - val_precision: 0.6364 - val_recall: 0.9537 - val_specificity: 0.9650 - learning_rate: 7.8125e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 336ms/step - accuracy: 0.9692 - dice_coef: 0.7125 - iou: 0.8099 - loss: 0.2878 - precision: 0.6556 - recall: 0.9929 - specificity: 0.9670\n",
      "Epoch 41: val_iou did not improve from 0.79624\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 443ms/step - accuracy: 0.9692 - dice_coef: 0.7125 - iou: 0.8099 - loss: 0.2878 - precision: 0.6556 - recall: 0.9929 - specificity: 0.9670 - val_accuracy: 0.9651 - val_dice_coef: 0.6849 - val_iou: 0.7902 - val_loss: 0.7030 - val_precision: 0.6365 - val_recall: 0.9537 - val_specificity: 0.9651 - learning_rate: 7.8125e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - accuracy: 0.9695 - dice_coef: 0.7036 - iou: 0.8109 - loss: 0.2914 - precision: 0.6570 - recall: 0.9933 - specificity: 0.9673\n",
      "Epoch 42: val_iou did not improve from 0.79624\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 485ms/step - accuracy: 0.9695 - dice_coef: 0.7036 - iou: 0.8109 - loss: 0.2914 - precision: 0.6570 - recall: 0.9933 - specificity: 0.9673 - val_accuracy: 0.9646 - val_dice_coef: 0.6827 - val_iou: 0.7885 - val_loss: 0.6955 - val_precision: 0.6330 - val_recall: 0.9548 - val_specificity: 0.9645 - learning_rate: 7.8125e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step - accuracy: 0.9681 - dice_coef: 0.7189 - iou: 0.8093 - loss: 0.2847 - precision: 0.6554 - recall: 0.9934 - specificity: 0.9657\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 43: val_iou did not improve from 0.79624\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 491ms/step - accuracy: 0.9681 - dice_coef: 0.7189 - iou: 0.8093 - loss: 0.2847 - precision: 0.6554 - recall: 0.9934 - specificity: 0.9657 - val_accuracy: 0.9657 - val_dice_coef: 0.6881 - val_iou: 0.7926 - val_loss: 0.7192 - val_precision: 0.6417 - val_recall: 0.9519 - val_specificity: 0.9659 - learning_rate: 7.8125e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 0.9687 - dice_coef: 0.7187 - iou: 0.8101 - loss: 0.2857 - precision: 0.6566 - recall: 0.9929 - specificity: 0.9665\n",
      "Epoch 44: val_iou did not improve from 0.79624\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13037s\u001b[0m 9s/step - accuracy: 0.9687 - dice_coef: 0.7187 - iou: 0.8101 - loss: 0.2857 - precision: 0.6566 - recall: 0.9929 - specificity: 0.9665 - val_accuracy: 0.9661 - val_dice_coef: 0.6900 - val_iou: 0.7941 - val_loss: 0.7289 - val_precision: 0.6448 - val_recall: 0.9508 - val_specificity: 0.9664 - learning_rate: 3.9063e-06\n",
      "Epoch 45/100\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9696 - dice_coef: 0.7270 - iou: 0.8164 - loss: 0.2794 - precision: 0.6683 - recall: 0.9929 - specificity: 0.9674\n",
      "Epoch 45: val_iou did not improve from 0.79624\n",
      "\u001b[1m1398/1398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 150ms/step - accuracy: 0.9696 - dice_coef: 0.7270 - iou: 0.8164 - loss: 0.2794 - precision: 0.6683 - recall: 0.9929 - specificity: 0.9674 - val_accuracy: 0.9656 - val_dice_coef: 0.6874 - val_iou: 0.7920 - val_loss: 0.7190 - val_precision: 0.6405 - val_recall: 0.9522 - val_specificity: 0.9657 - learning_rate: 3.9063e-06\n",
      "Epoch 45: early stopping\n",
      "Restoring model weights from the end of the best epoch: 35.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/15 06:31:50 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/06/15 06:32:07 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\regin\\AppData\\Local\\Temp\\tmpa8txwarp\\model, flavor: keras). Fall back to return ['keras==3.10.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run geral_teste1 at: http://localhost:5000/#/experiments/793009514534895904/runs/892c93460ef246e495c99fd6aa449d01\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/793009514534895904\n",
      "Treinamento concluído em 24437.83 segundos\n",
      "Carregando o melhor modelo salvo do arquivo 'melhor_modelo.keras'...\n",
      "Salvando o modelo no formato de pasta (TensorFlow SavedModel) em 'C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet'...\n",
      "INFO:tensorflow:Assets written to: C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet/modelo_melhor_treino_gerais'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='input_layer_3')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1328030149968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030154192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030154576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030149584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030150160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030147856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030154000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030152272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030149392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030151504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030150544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030150736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030151312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030149776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030153040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030151888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030152464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030152656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030152080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030153808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030152848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030150352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030150928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030153424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030151120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030153616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070635920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070634768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030151696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1328030153232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070632272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070636304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070636112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070627472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070637648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070636880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070637264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070637072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070636496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070638608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070638416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070638032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070637456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070635728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070639568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070638992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070639184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070639376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070636688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070640336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070638224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070639952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070640144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070637840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070641296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070640720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070640912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070641104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070638800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070642256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070641680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070641872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070641488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070643024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070642448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070640528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070642640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070642832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070642064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1333070639760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863267600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863268752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863268560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863268176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863269328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863267792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863268944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863269136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863267408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863270288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863269712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863269904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863270096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863267984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863271248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863270672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863270864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863271056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863268368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863272208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863271632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863271824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863272016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863269520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863273168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863272592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863272784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863272976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863270480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863274128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863273552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863273744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863273936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863271440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863275088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863274512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863274704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863274896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863272400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863276048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863275472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863275664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863275856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863273360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863277008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863276432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863276624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863276816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863274320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863277968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863277392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863277584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863277776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863275280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863278928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863278352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863278544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863278736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863276240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863279888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863279312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863279504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863279696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863277200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863280848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863280272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863280464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863280656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863278160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863281808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863281232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863281424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863281616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863279120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863282768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863282192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863282384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863282000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863283536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863282960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863281040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863283152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863283344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863282576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327863280080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864217872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864219024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864218832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864218448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864219600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864218064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864219216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864219408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864217680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864220560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864219984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864220176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864220368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864218256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864221520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864220944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864221136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864221328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864218640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864222480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864221904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864222096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864222288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864219792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864223440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864222864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864223056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864223248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864220752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864224400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864223824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864224016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864224208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864221712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864225360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864224784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864224976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864225168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864222672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864226320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864225744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864225936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864226128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864223632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864227280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864226704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864226896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864227088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864224592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864228240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864227664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864227856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864228048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864225552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864229200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864228624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864228816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864229008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864226512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864230160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864229584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864229776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864229968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864227472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864231120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864230544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864230736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864230928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864228432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864232080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864231504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864231696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864231888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864229392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864233040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864232464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864232656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864232272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864233808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864233232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864231312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864233424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864233616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864232848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327864230352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865037072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865038224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865038032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865037648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865038800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865037264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865038416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865038608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865036880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865039760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865039184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865039376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865039568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865037456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865040720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865040144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865040336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865040528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865037840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865041680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865041104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865038992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865042448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865041872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865042832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865042640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865043216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865043024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865043600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865043408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865043984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865043792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865044368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865044176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865044752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865044560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865045136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865044944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865045520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865045328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865045904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865045712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865046288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865046096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865046672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865046480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865047056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865046864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865047440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865047248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865047824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1327865047632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Processo concluído com sucesso!\n"
     ]
    }
   ],
   "source": [
    "crop_size = 128\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "gamma = 2\n",
    "augment=True\n",
    "\n",
    "# Caminhos de dados\n",
    "train_image_dir = \"d:/SIMGEO 2025/DATASETS/GERAL/IMGS\"\n",
    "train_mask_dir = \"d:/SIMGEO 2025/DATASETS/GERAL/MASKS\"\n",
    "\n",
    "model = train_model(train_image_dir=train_image_dir, \n",
    "                    train_mask_dir=train_mask_dir, \n",
    "                    epochs=epochs, \n",
    "                    crop_size=crop_size, \n",
    "                    batch_size=batch_size, \n",
    "                    lr=lr, \n",
    "                    gamma=gamma,\n",
    "                    experiment_name='Modelos_gerais',\n",
    "                    run_name='geral_teste1', \n",
    "                    model_name='modelo_melhor_treino_gerais', \n",
    "                    saving_dir='C:/Users/regin/Documents/ORF/ricardofranco/rede_neural_unet',\n",
    "                    augment=augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_georeferenced_mask(predicted_mask, reference_image_path, output_path, threshold):\n",
    "\n",
    "    # Abrir a imagem de referência para obter informações georreferenciadas\n",
    "    ref_ds = gdal.Open(reference_image_path)\n",
    "    if ref_ds is None:\n",
    "        raise FileNotFoundError(f\"Não foi possível abrir a imagem de referência: {reference_image_path}\")\n",
    "    \n",
    "    # Obter informações georreferenciadas\n",
    "    geo_transform = ref_ds.GetGeoTransform()\n",
    "    projection = ref_ds.GetProjection()\n",
    "    ref_ds = None  # Fechar o dataset da referência\n",
    "    \n",
    "    # Binarizar a máscara predita com um limite (threshold) de 0.5\n",
    "    binary_mask = (predicted_mask >= threshold).astype(np.uint8)\n",
    "\n",
    "    # Criar um arquivo TIFF georreferenciado\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    out_ds = driver.Create(\n",
    "        output_path, \n",
    "        binary_mask.shape[1], \n",
    "        binary_mask.shape[0], \n",
    "        1,  # Apenas um canal\n",
    "        gdal.GDT_Byte  # Tipo de dado Byte (0 ou 1)\n",
    "    )\n",
    "    if out_ds is None:\n",
    "        raise RuntimeError(f\"Não foi possível criar o arquivo: {output_path}\")\n",
    "    \n",
    "    # Aplicar informações georreferenciadas\n",
    "    out_ds.SetGeoTransform(geo_transform)\n",
    "    out_ds.SetProjection(projection)\n",
    "    \n",
    "    # Escrever a máscara binária no arquivo\n",
    "    out_ds.GetRasterBand(1).WriteArray(binary_mask)\n",
    "    \n",
    "    # Fechar o dataset de saída\n",
    "    out_ds.FlushCache()\n",
    "    out_ds = None\n",
    "    \n",
    "    print(f\"Máscara predita salva como TIFF georreferenciado em: {output_path}\")\n",
    "\n",
    "def crop_image(image_path, crop_size):\n",
    "    # Abrir a imagem com GDAL\n",
    "    img_ds = gdal.Open(image_path)\n",
    "    if img_ds is None:\n",
    "        raise FileNotFoundError(f\"Não foi possível abrir a imagem: {image_path}\")\n",
    "\n",
    "    # Dimensões da imagem\n",
    "    img_width = img_ds.RasterXSize\n",
    "    img_height = img_ds.RasterYSize\n",
    "    img_bands = img_ds.RasterCount\n",
    "\n",
    "    # Inicializar listas para armazenar os blocos e coordenadas\n",
    "    crops = []\n",
    "    coords = []\n",
    "\n",
    "    # Iterar sobre a imagem em blocos de crop_size x crop_size\n",
    "    for y in range(0, img_height - crop_size + 1, crop_size):\n",
    "        for x in range(0, img_width - crop_size + 1, crop_size):\n",
    "            if img_bands > 3:  # Ajustar para imagens multibanda\n",
    "                crop = np.stack(\n",
    "                    [\n",
    "                        img_ds.GetRasterBand(1).ReadAsArray(x, y, crop_size, crop_size),  # Banda 1 (R)\n",
    "                        img_ds.GetRasterBand(2).ReadAsArray(x, y, crop_size, crop_size),  # Banda 2 (G)\n",
    "                        img_ds.GetRasterBand(3).ReadAsArray(x, y, crop_size, crop_size),  # Banda 3 (B)\n",
    "                    ],\n",
    "                    axis=-1,  # Combinar no formato (H, W, C)\n",
    "                )\n",
    "\n",
    "            # Adicionar o bloco e suas coordenadas à lista\n",
    "            crops.append(crop)\n",
    "            coords.append((y, x))\n",
    "\n",
    "    # Fechar o dataset GDAL\n",
    "    img_ds = None\n",
    "\n",
    "    return np.array(crops), coords\n",
    "\n",
    "def reconstruct_image(predicted_crops, coords, original_shape, crop_size):\n",
    "    # Criar matriz para reconstrução e mapa de contagem\n",
    "    reconstructed = np.zeros(original_shape[:2], dtype=np.float32)\n",
    "    count_map = np.zeros(original_shape[:2], dtype=np.float32)\n",
    "\n",
    "    # Combinar blocos preditos na posição correta\n",
    "    for (i, j), crop in zip(coords, predicted_crops):\n",
    "        reconstructed[i:i+crop_size, j:j+crop_size] += crop.squeeze()\n",
    "        count_map[i:i+crop_size, j:j+crop_size] += 1\n",
    "\n",
    "    return reconstructed\n",
    "\n",
    "def predict_and_save(test_imgs_dir, model, model_save_path, crop_size, threshold, save_model=False):\n",
    "\n",
    "    imgs = os.listdir(test_imgs_dir)\n",
    "\n",
    "    for img in imgs:\n",
    "        if not img.endswith('.tif'):\n",
    "            print(f'O arquivo {img} não é uma imagem TIF')\n",
    "        else:\n",
    "            test_img_path = os.path.join(test_imgs_dir, img)\n",
    "\n",
    "            # Carregar e recortar a imagem de teste\n",
    "            test_image_crops, test_coords = crop_image(test_img_path, crop_size=crop_size)\n",
    "            test_image = cv2.imread(test_img_path) / 255.0\n",
    "            \n",
    "            # Prever cada bloco\n",
    "            predicted_crops = model.predict(test_image_crops)\n",
    "\n",
    "            # Reconstruir a máscara predita no tamanho original\n",
    "            predicted_mask_full = reconstruct_image(predicted_crops, test_coords, test_image.shape, crop_size=crop_size)\n",
    "            print(max(np.unique(predicted_mask_full)), min(np.unique(predicted_mask_full)))\n",
    "            # Salvar a máscara predita\n",
    "            predicted_mask_path = f\"D:/ORF/dados-rf-cma/pista/resultados-testes/predicted_mask_{img}\"\n",
    "            save_georeferenced_mask(predicted_mask_full, test_img_path, predicted_mask_path, threshold=threshold)\n",
    "    # if save_model == True:\n",
    "    #     # Salvar o modelo treinado\n",
    "    #     model.save(model_save_path+'.')\n",
    "    #     print(f\"Modelo treinado salvo em: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_save(test_imgs_dir=test_image_path, model=model, model_save_path=model_save_path, crop_size=crop_size, threshold=0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simgeo_projeto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
